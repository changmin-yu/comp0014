\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Probability and Statistics II}
\author{Changmin Yu}
\date{February 2020}
\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}

\maketitle
\section{Maximum Likelihood Estimation}
\textbf{Likelihood} is a concept useful for comparing alternative models with each other. It describes the probability of the observed data being generated by an underlying model (of a random process) having certain parameters. In other words, it is a measure of goodness of fit between the data and the proposed model with given parameter values. More formally:
\begin{itemize}
    \item Given data $y\in\mathbb{R}^n$ as realisations of a random variable $Y$, specify its density $f(y; \theta)$ up to some unknown vector of parameters $\theta \in \Theta \subset \mathbb{R}^d$, where $\Theta$ is the parameter space.
    \item Define the \textbf{likelihood function} as a function of the parameters $\theta$:
    \begin{equation}
        L(\theta) = L(\theta; y) = c(y)f(y; \theta)
    \end{equation}
    where $c(y)$ is some unknown constant for normalisation
    \item The maximum likelihood estimator (MLE) of $\theta$, $\hat{\theta}$, is the value of parameters such that $\hat{\theta}$ maximises $L(\theta)$.
    \begin{equation}
        \hat{\theta} = \argmax_{\theta}L(\theta; y)
    \end{equation}
    \item It is common to work with the log-likelihood function.
\end{itemize}
\section{Maximum A Posteriori Estimation}
An alternative way of evaluating the quality of models or selecting between multiple alternative parameter values is to calculate the opposite: the probability of a model's parameters given the data observed. Then the most probable values can be selected. This probability can be calculated from the likelihood using Bayes' theorem:
\begin{itemize}
    \item Bayes' theorem:
    \begin{equation}
        f(x|y) = \frac{f(y|x)f(x)}{f(y)} = \frac{f(y|x)f(x)}{\int f(y|x)f(x)dx}
    \end{equation}
    \item Given data $y$ and parametric density function $f(y; \theta)$, the maximum a posteriori (MAP) estimator of $\theta$ given its prior belief $g(\theta)$ is
    \begin{equation}
        \hat{\theta}_{\text{MAP}} = \argmax_{\theta}\frac{f(y; \theta)g(\theta)}{\int f(y; \theta)g(\theta)d\theta}
    \end{equation}
\end{itemize}
Note that in order to calculate the posterior probability $f(\theta;y)$, we need to have a notion of how likely the parameter values are to occur in general in the wild i.e. the probability distribution of parameter values - $g(\theta)$. This is called the \textbf{prior}. What is happening above is that this prior belief, $g(\theta)$, is being adjusted in light of the data observed, $y$, to yield new probability values, $f(\theta;y)$; then the value with maximum probability is selected.
\section{Ordinary Least Squares}
Linear regression is a simple statistical model where the output variable(s) $Y$ are modeled by / predicted to be  a linear combination of input variable(s) $X$, plus some unknown error. In other words:
\begin{itemize}
    \item $Y = X\beta + \epsilon$
\end{itemize}
where $\beta$ is a matrix of coefficients. These coefficients are typically selected by minimizing the sum of squared differences between the predicted and actual values, i.e. via ordinary least squares (OLS):
\begin{itemize}
    \item OLS estimator
    \begin{equation}
        \hat{\beta} = \argmin_{\beta\in\mathbb{R}^p}||Y-X\beta||^2  = (X^TX)^{-1}X^TY
    \end{equation}
    Note that this is also an orthogonal projection onto the column space of $X$.
\end{itemize}

\end{document}